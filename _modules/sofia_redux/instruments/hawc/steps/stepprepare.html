<!DOCTYPE html>

<html lang="en" data-content_root="../../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sofia_redux.instruments.hawc.steps.stepprepare &#8212; sofia_redux v1.3.4.dev38+g92ea2f4</title>
    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/bootstrap-sofia.css?v=3fe2c07e" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/plot_directive.css" />
    
    <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../../../_static/documentation_options.js?v=6aa39468"></script>
    <script src="../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script type="text/javascript" src="../../../../../_static/sidebar.js"></script>
    <script type="text/javascript" src="../../../../../_static/copybutton.js"></script>
    <link rel="icon" href="../../../../../_static/redux.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,600' rel='stylesheet' type='text/css'/>

  </head><body>
<div class="topbar">
  <a class="brand" title="Documentation Home" href="../../../../../index.html"><span id="logotext1">SOFIA</span><span id="logotext2">Redux</span><span id="logotext3">:docs</span></a>
  <ul>
    <li><a class="homelink" title="SOFIA Homepage" href="https://irsa.ipac.caltech.edu/Missions/sofia.html"></a></li>
    <li><a title="General Index" href="../../../../../genindex.html">Index</a></li>
    <li><a title="Module Index" href="../../../../../py-modindex.html">Modules</a></li>
    <li>
      
      
<form action="../../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
      
    </li>
  </ul>
</div>

<div class="related">
    <h3>Navigation</h3>
    <ul>
      <li>
	<a href="../../../../../index.html">sofia_redux v1.3.4.dev38+g92ea2f4</a>
	 &#187;
      </li>
      <li><a href="../../../../index.html" accesskey="U">Module code</a> &#187;</li>
      
       
    </ul>
</div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sofia_redux.instruments.hawc.steps.stepprepare</h1><div class="highlight"><pre>
<span></span># Licensed under a 3-clause BSD style license - see LICENSE.rst
&quot;&quot;&quot;Raw data preparation pipeline step.&quot;&quot;&quot;

from astropy import log
from astropy.io import fits
import numpy as np

from sofia_redux.instruments.hawc.stepparent import StepParent

__all__ = [&#39;StepPrepare&#39;]


<div class="viewcode-block" id="StepPrepare">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.html#sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare">[docs]</a>
class StepPrepare(StepParent):
    &quot;&quot;&quot;
    Prepare input file for processing.

    This step separates the flux column into separate columns
    for R and T arrays. It may also fill in missing columns,
    and reformat or rename existing columns to conventions expected
    by subsequent pipeline steps.

    Currently steprepare assumes the input array has dimensions 64 x
    41, and just splits it into two 32 x 41 sub-arrays which are
    named &#39;R Array&#39; (columns 1-32) and &#39;T Array&#39; (columns 33-64) in the
    output file. If a column named &#39;Chop Offset&#39; is not present in the
    input data, it is created, replicating the column given in the
    parameter &#39;chpoff&#39;. If a column named &#39;HWP Angle&#39; is not present,
    then it is generated by multiplying the &#39;hwpcounts&#39; column by a
    conversion factor obtained from the parameter &#39;hwpconv&#39;.

    Input for this step is a raw FITS file, containing the primary
    HDU (with only the header), the Configuration HDU, and a binary
    table (&#39;Timestream&#39;) containing the arrays, chop and HWP
    signals, etc. The output for this step is the same as the input,
    with the original flux array removed and &#39;R Array&#39;, &#39;T Array&#39; columns
    added in its place.
    &quot;&quot;&quot;
<div class="viewcode-block" id="StepPrepare.setup">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.html#sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.setup">[docs]</a>
    def setup(self):
        &quot;&quot;&quot;
        Set parameters and metadata for the pipeline step.

        Output files have PRODTYPE = &#39;prepare&#39;, and are named with
        the step abbreviation &#39;PRE&#39;.

        Parameters defined for this step are:

        detcounts : str
            Name of the table column containing the detector flux
            values for R/T arrays. Usually set to &#39;SQ1 Feedback&#39;.
        hwpcounts : str
            Name of the table column containing the HWP counts
            (only used if column &#39;HWP Angle&#39; is not present).
        hwpconv : float
            Value to convert hwpcounts to HWP Angle
            (only used if column &#39;HWP Angle&#39; is not present).
        labmode : bool
            If set, data is assumed to be missing header keywords
            and data columns for on-sky data. These columns are filled
            in with default values to allow processing to continue.
        replacenod : bool
            If set, the &#39;Nod Offset&#39; column will be replaced by
            a calculation based on RA/DEC. If not set, the original
            column will be used. replacenod=True is recommended.
        chpoffsofiars : bool
            If set, the &#39;Chop Offset&#39; column will be calculated from
            the SofiaChopR/S columns.
        colrename : str
            List of data columns to rename, as a &#39;|&#39; separated string.
            The format is: &#39;oldname1-&gt;newname1|oldname2-&gt;newname2|...&#39;.
        coldelete : list of str
            List of data columns to delete.
        pixscalist : list of float
            List of PIXSCAL values, one for each HAWC waveband. Used
            to write a FITS header keyword.
        traceshift : int
            Number of samples to shift the data by. If non-zero, all
            columns except Timestamp, R Array, and T Array will by
            shifted by this amount, in order to align the instrument
            readouts with the metadata.
        removedropouts : bool
            If set, data dropouts (with RA = Dec = 0) will be removed
            from the file.
        &quot;&quot;&quot;

        # Name of the pipeline reduction step
        self.name = &#39;prepare&#39;
        self.description = &#39;Prepare Data Arrays&#39;

        # Shortcut for pipeline reduction step and identifier for
        # saved file names.
        self.procname = &#39;pre&#39;

        # Clear Parameter list
        self.paramlist = []

        # Append parameters
        self.paramlist.append([&#39;detcounts&#39;, &#39;SQ1 Feedback&#39;,
                               &#39;Name of the input fits binary table &#39;
                               &#39;column containing the detector flux &#39;
                               &#39;values for R/T arrays&#39;])
        self.paramlist.append([&#39;hwpcounts&#39;, &#39;hwpCounts&#39;,
                               &#39;Name of the input fits column containing &#39;
                               &#39;the HWP counts (only used if column &#39;
                               &#39;&quot;HWP Angle&quot; is not present)&#39;])
        self.paramlist.append([&#39;hwpconv&#39;, (360. / 1440.),
                               &#39;Value to convert hwpcounts to HWP &#39;
                               &#39;Angles (only used if column &quot;HWP Angle&quot; &#39;
                               &#39;is not present)&#39;])
        self.paramlist.append([&#39;labmode&#39;, False,
                               &#39;If TRUE (processing lab data), will &#39;
                               &#39;fill in with zeros a few columns and &#39;
                               &#39;keywords that are important for the DRP&#39;])
        self.paramlist.append([&#39;replacenod&#39;, True,
                               &#39;If TRUE will replace Nod Offset by &#39;
                               &#39;calculation based on RA/DEC. If False, &#39;
                               &#39;use original column (has problems)&#39;])
        self.paramlist.append([&#39;chpoffsofiars&#39;, True,
                               &#39;If TRUE will calculate Chop Offset &#39;
                               &#39;based on SofiaChopR/S. If False, use &#39;
                               &#39;colrename to specify column to use&#39;])
        self.paramlist.append([&#39;colrename&#39;, &#39;&#39;,
                               &#39;List of data columns to rename, &#39;
                               &#39;The format is: &#39;
                               &#39;oldname1-&gt;newname1|oldname2-&gt;newname2|...&#39;])
        self.paramlist.append([&#39;coldelete&#39;, [&quot;hwpA&quot;, &quot;hwpB&quot;],
                               &#39;List of data columns to delete. &#39;
                               &#39;The format is: [&quot;column1&quot;, &quot;column2&quot;, ...]&#39;])
        self.paramlist.append([&#39;pixscalist&#39;, [2.6, 4.0, 4.0, 6.8, 9.0],
                               &#39;List for PIXSCAL values for each band&#39;])
        self.paramlist.append([&#39;traceshift&#39;, 0,
                               &#39;Number of samples to shift the data &#39;
                               &#39;(default is 0=no shift)&#39;])
        self.paramlist.append([&#39;removedropouts&#39;, False,
                               &#39;Remove data dropouts (i.e. data with &#39;
                               &#39;RA==Dec==0 - default is False)&#39;])</div>


<div class="viewcode-block" id="StepPrepare.read_pixscal">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.html#sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.read_pixscal">[docs]</a>
    def read_pixscal(self):
        &quot;&quot;&quot;
        Read a pixel scale value from the parameters.

        The parameters are expected to be defined as a list, with
        one entry for each HAWC band. The correct value for the
        input data is selected from the list.

        Returns
        -------
        float
            The pixel scale.
        &quot;&quot;&quot;
        pixscalist = self.getarg(&#39;pixscalist&#39;)
        waveband = self.datain.getheadval(&#39;spectel1&#39;)
        bands = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;]
        try:
            idx = bands.index(waveband[-1])
        except (ValueError, IndexError):
            # waveband not in list
            msg = &#39;Cannot parse waveband=%s&#39; % waveband
            log.error(msg)
            raise ValueError(msg)
        try:
            pixscal = pixscalist[idx]
        except IndexError:
            msg = &#39;Need pixscal values for all wavebands&#39;
            log.error(msg)
            raise IndexError(msg)
        return pixscal</div>


<div class="viewcode-block" id="StepPrepare.run">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.html#sofia_redux.instruments.hawc.steps.stepprepare.StepPrepare.run">[docs]</a>
    def run(self):
        &quot;&quot;&quot;
        Run the data reduction algorithm.

        Because this step is single-in, single-out (SISO),
        self.datain must be a DataFits object. The output
        is also a DataFits object, stored in self.dataout.

        The process is:

        1. Read the input data table.
        2. Split the flux array into R and T subarrays.
        3. Rename columns specified in the &#39;colrename&#39; parameter.
        4. Remove any columns specified in the &#39;coldelete&#39; parameter.
        5. Add any columns necessary from a housekeeping data file.
        6. Calculate chop and nod offsets and HWP angles as necessary.
        7. Shift metadata columns to match flux samples if necessary.
        8. Store the data. Input data is modified in place.
        &quot;&quot;&quot;

        # Read input parameters
        tstream = self.getarg(&#39;detcounts&#39;)
        hwpcnts = self.getarg(&#39;hwpcounts&#39;)
        hwpconv = self.getarg(&#39;hwpconv&#39;)
        labmode = self.getarg(&#39;labmode&#39;)
        replacenodoffset = self.getarg(&#39;replacenod&#39;)
        chpoffsofiars = self.getarg(&#39;chpoffsofiars&#39;)
        colrename_str = self.getarg(&#39;colrename&#39;)
        coldelete = self.getarg(&#39;coldelete&#39;)
        traceshift = self.getarg(&#39;traceshift&#39;)
        removedropouts = self.getarg(&#39;removedropouts&#39;)

        # Convert colrename to a dictionary
        # colrename[&#39;oldname&#39;]=newname
        colrename = {}
        if len(colrename_str) &gt; 0:
            for colrep in colrename_str.split(&#39;|&#39;):
                colspl = colrep.split(&#39;-&gt;&#39;)
                # don&#39;t if &#39;-&gt;&#39; not found
                if len(colspl) &lt; 2:
                    log.warning(&#39;Invalid colrename item = &lt;%s&gt;&#39; % colrep)
                else:
                    colrename[colspl[0].strip()] = colspl[1].strip()

        # Names of the columns in the table input
        rawnames = self.datain.table.names

        # Handle Detector data
        # Get the timestream data into coltstream
        coltstream = None
        for i, c in enumerate(rawnames):
            if c == tstream:
                coltstream = self.datain.table.field(c)
        if tstream not in rawnames:
            msg = &quot;Table %s not found in raw data&quot; % tstream
            log.error(msg)
            raise ValueError(msg)

        # Splitting the input array into R an T
        rarray = coltstream[:, :, :64]
        tarray = coltstream[:, :, 64:]
        # array dimensions (R must match T)
        nsamp, nrow, ncol = rarray.shape

        # Define new columns to be written -&gt; COL
        # Add R array and T array
        col = [fits.Column(name=&#39;R array&#39;, unit=&#39;counts&#39;, array=rarray,
                           format=&#39;%dD&#39; % (nrow * ncol),
                           dim=&#39;(%d,%d)&#39; % tuple([ncol, nrow])),
               fits.Column(name=&#39;T array&#39;, unit=&#39;counts&#39;, array=tarray,
                           format=&#39;%dD&#39; % (nrow * ncol),
                           dim=&#39;(%d,%d)&#39; % tuple([ncol, nrow]))]

        # Add ColReplace columns only if they are
        # not originally in the input file
        for oldcname in colrename.keys():
            newcname = colrename[oldcname]

            # Make sure it&#39;s not already in
            if newcname not in rawnames:
                # Error message if oldcname not in rawnames
                if oldcname not in rawnames:
                    msg = &quot;Column %s not found in raw data&quot; % oldcname
                    log.error(msg)
                    raise ValueError(msg)

                # Search for column to replace
                colunit = None
                coldata = None
                for i in range(len(rawnames)):
                    if rawnames[i] == oldcname:
                        coldata = self.datain.table.field(i)
                        if len(coldata.shape) &gt; 1:
                            # raise error for multi-dim --
                            # needs special handling
                            msg = &quot;Column %s is multidimensional; &quot; \
                                  &quot;cannot rename&quot; % oldcname
                            log.error(msg)
                            raise ValueError(msg)
                        colunit = self.datain.table.columns.units[i]
                col.append(fits.Column(name=newcname, unit=colunit,
                                       array=coldata,
                                       format=&#39;1E&#39;))
                log.debug(&#39;Renaming column %s to %s&#39; %
                          (oldcname, newcname))
                # mark column for deletion
                colrename[oldcname] = &#39;delete&#39;
            else:
                log.warning(&quot;Column %s already in output data, &quot;
                            &quot;replace is ignored&quot; % newcname)

        # Add HWP angle column only if it is not
        # originally in input file (convert to degrees)
        if &#39;HWP Angle&#39; not in rawnames:
            if hwpcnts not in rawnames:
                msg = &quot;Column %s not found in raw data&quot; % hwpcnts
                log.error(msg)
                raise ValueError(msg)
            colhwpang = None
            for c in rawnames:
                if c == hwpcnts:
                    colhwpang = self.datain.table.field(c) * hwpconv
            col.append(fits.Column(name=&#39;HWP Angle&#39;,
                                   unit=&#39;degrees&#39;, array=colhwpang,
                                   format=&#39;1E&#39;))

        # Delete old Nod Offset column and make new
        # column to be replaced by Nod Offset based on RA and DEC
        if replacenodoffset:
            # Remove existing Nod Offset columns in original of new columns
            if &#39;Nod Offset&#39; in rawnames:
                self.datain.table.columns.del_col(&#39;Nod Offset&#39;)
            for c in col:
                if c.name == &#39;Nod Offset&#39;:
                    col.remove(c)

            # Append new nod offset column
            col.append(fits.Column(name=&#39;Nod Offset&#39;, unit=&#39;arcseconds&#39;,
                                   array=np.zeros(nsamp),
                                   format=&#39;1E&#39;))
        else:
            log.warning(&#39;Using original Nod Offset column &#39;
                        &#39;(beware of problems with this column)&#39;)

        # Prepare a column for Chop Offset based on SofiaChopR and S signals
        if chpoffsofiars:
            col.append(fits.Column(name=&#39;Chop Offset&#39;, unit=&#39;arcseconds&#39;,
                                   array=np.zeros(nsamp),
                                   format=&#39;1E&#39;))
        else:
            log.warning(&#39;Using chpoffsofiars = False. Use &#39;
                        &#39;colrename to define Chop Offset &#39;
                        &#39;with the correct column.&#39;)

        # Remove old columns

        # Removes the original array (SQ1 Feedback)
        # from the input data (will keep only &quot;R Array&quot; and &quot;T Array&quot;)
        self.datain.table.columns.del_col(tstream)

        # Remove other columns from the input data
        for i in range(len(coldelete)):
            if coldelete[i] in rawnames:
                # Added B/C tabhdu = . . . below would crash
                self.datain.table.columns.del_col(coldelete[i])

        # Remove columns from colreplace
        for oldcname in colrename.keys():
            if colrename[oldcname] == &#39;delete&#39;:
                self.datain.table.columns.del_col(oldcname)

        # Replicates the input data to generate the output
        self.dataout = self.datain.copy()
        self.dataout.filename = self.datain.filename

        # Define the original columns to be written
        origcols = self.datain.table.columns

        # Make new table
        newcols = fits.ColDefs(col)
        log.debug(&quot;New Cols = &quot; + repr(newcols.names))
        log.debug(&quot;Orig Cols = &quot; + repr(origcols.names))
        tbhdu = fits.BinTableHDU.from_columns(newcols + origcols)
        # assuming there is only one binary table
        tabdataname = self.datain.tabnames[0]
        self.dataout.tableset(tbhdu.data, tabdataname, tbhdu.header)

        # Modify data

        # Configuration for LAB MODE
        if labmode:
            log.warning(&#39;Running in LAB MODE. Will fill in &#39;
                        &#39;important columns and keywords &#39;
                        &#39;with random values&#39;)

            # Fill in certain columns with zeros if those are NaNs
            # This is important for stepdemod and stepwcs
            for f in [&#39;Azimuth Error&#39;, &#39;Elevation Error&#39;,
                      &#39;Azimuth&#39;, &#39;Elevation&#39;, &#39;Array VPA&#39;]:
                t = np.where(np.isnan(self.dataout.table.field(f)))
                self.dataout.table.field(f)[t] = np.float64(0.0)
                if len(t[0]) &gt; 0:
                    log.warning(&#39;%d values in %s column were &#39;
                                &#39;NaNs and were substituted &#39;
                                &#39;by zeros&#39; % (len(t[0]), f))

            # Fill in important keywords
            self.dataout.setheadval(&quot;OBSRA&quot;, 1.0)
            self.dataout.setheadval(&quot;OBSDEC&quot;, 1.0)

        # Adapt PIXSCAL keywords
        pixscal = self.read_pixscal()
        self.dataout.setheadval(&quot;PIXSCAL&quot;, pixscal)

        # Remove an engineering keyword
        self.dataout.delheadval(&#39;XPADDING&#39;)

        # Replace Nod Offset column by a computation
        # of offsets based on RA and DEC
        if replacenodoffset:
            # Reading the reference position from the header
            try:
                # convert to degrees
                telra = self.datain.getheadval(&quot;TELRA&quot;) * 15.0
                teldec = self.datain.getheadval(&quot;TELDEC&quot;)
            except KeyError:
                log.warning(&#39;TELRA AND TELDEC NOT DEFINED IN &#39;
                            &#39;INPUT DATA. WILL USE OBSRA AND OBSDEC &#39;
                            &#39;INSTEAD (not recommended).&#39;)
                # convert to degrees
                telra = self.datain.getheadval(&quot;OBSRA&quot;) * 15.0
                teldec = self.datain.getheadval(&quot;OBSDEC&quot;)

            # Get the RA and DEC columns
            try:
                # convert to degrees
                ra_col = self.dataout.table.field(&#39;RA&#39;) * 15.0
                dec_col = self.dataout.table.field(&#39;DEC&#39;)
            except KeyError:
                msg = &#39;RA and DEC columns not found in data&#39;
                log.warning(msg)
                raise ValueError(msg)

            # Calculate new nod offset column
            cos_dec = np.cos(teldec * (np.pi / 180.0))
            # convert to arcseconds
            offra = ((ra_col - telra) * cos_dec) * 3600.0
            offdec = (dec_col - teldec) * 3600.0
            self.dataout.table[&#39;Nod Offset&#39;] = np.sqrt(offra**2. + offdec**2.)
        else:
            log.warning(&#39;Using original Nod Offset column &#39;
                        &#39;(beware of problems with this column)&#39;)

        # Computing Chop Offset based on SofiaChopR and S signals
        if chpoffsofiars:
            try:
                chpr = self.dataout.table.field(&#39;SofiaChopR&#39;)
                chps = self.dataout.table.field(&#39;SofiaChopS&#39;)
                chpsync = self.dataout.table.field(&#39;SofiaChopSync&#39;)
            except KeyError:
                msg = &#39;SofiaChopR and S columns not found in data&#39;
                log.warning(msg)
                raise ValueError(msg)

            # Make sure all chop signals are centered at 0,
            # by subtracting the mean value for each one
            chpr_col = chpr - np.mean(chpr)
            chps_col = chps - np.mean(chps)
            chpsync_col = chpsync - np.mean(chpsync)
            log.debug(&#39;Mean Chop R: {}&#39;.format(np.mean(chpr)))
            log.debug(&#39;Mean Chop S: {}&#39;.format(np.mean(chps)))
            log.debug(&#39;Mean Chop Sync {}&#39;.format(np.mean(chpsync)))

            # Normalize all signals by dividing them by
            # the amplitude (assumed to be (max - min)/2)
            low, high = np.percentile(chpr_col, [5, 95])
            chpr_col = chpr_col / ((high - low) / 2.)
            log.debug(&#39;Low, high Chop R: {}, {}&#39;.format(low, high))
            low, high = np.percentile(chps_col, [5, 95])
            chps_col = chps_col / ((high - low) / 2.)
            log.debug(&#39;Low, high Chop S: {}, {}&#39;.format(low, high))
            low, high = np.percentile(chpsync_col, [5, 95])
            chpsync_col = chpsync_col / ((high - low) / 2.)
            log.debug(&#39;Low, high Chop Sync: {} {}&#39;.format(low, high))

            # check for missing chop sync
            if np.abs(low) &lt; 1.0 and np.abs(high) &lt; 1.0:
                log.warning(&#39;Chop Sync signal may be missing; &#39;
                            &#39;check chop offsets&#39;)

            # Add ChopR and ChopS with ChopSync, to test
            # which of those are in phase with ChopSync
            # Here we assume the chop signals have a
            # phase difference of either 0 or 180 deg
            addchpr_sync = chpr_col + chpsync_col
            addchps_sync = chps_col + chpsync_col
            stdval = [np.nanstd(chpr_col),
                      np.nanstd(addchpr_sync),
                      np.nanstd(chps_col),
                      np.nanstd(addchps_sync)]
            for i, v in enumerate(stdval):
                if not np.isfinite(v):
                    stdval[i] = 1e6
            log.debug(&#39;Std.Dev for ChopR, ChopR+Sync; &#39;
                      &#39;ChopS, ChopS+Sync: &#39;
                      &#39;{}, {}; {}, {}&#39;.format(*stdval))

            # Add chopr and chops to Chop Offset
            if (stdval[1] &gt; stdval[0]) and \
                    (stdval[3] &gt; stdval[2]):
                self.dataout.table[&#39;Chop Offset&#39;] = chpr + chps
                log.debug(&#39;Assigning Chop Offset to SofiaChopR + &#39;
                          &#39;SofiaChopS (both are in phase &#39;
                          &#39;with sofiaChopSync)&#39;)
            elif (stdval[1] &gt; stdval[0]) and \
                    (stdval[3] &lt;= stdval[2]):
                self.dataout.table[&#39;Chop Offset&#39;] = chpr - chps
                log.debug(&#39;Assigning Chop Offset to &#39;
                          &#39;SofiaChopR - SofiaChopS (ChopS is out &#39;
                          &#39;of phase with sofiaChopSync)&#39;)
            elif (stdval[1] &lt;= stdval[0]) and \
                    (stdval[3] &gt; stdval[2]):
                self.dataout.table[&#39;Chop Offset&#39;] = chps - chpr
                log.debug(&#39;Assigning Chop Offset to &#39;
                          &#39;SofiaChopS - SofiaChopR (ChopR is out &#39;
                          &#39;of phase with sofiaChopSync)&#39;)
            elif (stdval[1] &lt;= stdval[0]) and \
                    (stdval[3] &lt;= stdval[2]):
                self.dataout.table[&#39;Chop Offset&#39;] = - chpr - chps
                log.debug(&#39;Assigning Chop Offset to &#39;
                          &#39;-1*(SofiaChopR + SofiaChopS) &#39;
                          &#39;(both are out of phase with sofiaChopSync)&#39;)
        else:
            log.warning(&#39;Using chpoffsofiars = False. Use colrename &#39;
                        &#39;to define Chop Offset with the correct column.&#39;)

        if &#39;Chop Offset&#39; not in self.dataout.table.names:
            msg = &#39;Problem with chop offset assignment&#39;
            log.error(msg)
            raise ValueError(msg)

        # Shift all data relative to R/T Array: Loop through all columns
        # (nsamp is number of samples)
        if traceshift:
            for colnam in self.dataout.table.names:
                # Skip if it&#39;s R/T Array or
                # Timestamp (these are the correct ones)
                if colnam in [&#39;Timestamp&#39;, &#39;R array&#39;, &#39;T array&#39;]:
                    continue
                # Shift data
                self.dataout.table[colnam][traceshift:] = \
                    self.dataout.table[colnam][:-traceshift]
            # Cut the table at the start
            self.dataout.table = self.dataout.table[traceshift:]

        # Remove data with auxiliary (SOFIA HK data) dropouts
        if removedropouts and not labmode:
            mask = np.where((self.dataout.table[&#39;RA&#39;] != 0.0)
                            &amp; (self.dataout.table[&#39;Dec&#39;] != 0.0))
            ndiff = len(self.dataout.table) - len(mask[0])
            if ndiff:
                log.warning(&#39;Remove Dropouts: Removed %d points&#39; % ndiff)
                log.warning(&#39;Dropouts: shorted data &#39;
                            &#39;from %d to %d points&#39; %
                            (len(self.dataout.table), len(mask[0])))
            self.dataout.tabdata[0] = self.dataout.table[mask]</div>
</div>

</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Page Contents</h3>


        </div>
      </div>
      <div class="clearer"></div>
    </div>
<footer class="footer">
  <p class="pull-right"> &nbsp;
    <a href="#">Back to Top</a></p>
  <p>
    &copy; Copyright 2024, SOFIA-USRA.<br/>
    Created using <a href="http://www.sphinx-doc.org/en/stable/">Sphinx</a> 7.2.6. &nbsp;
    Last built 05 Feb 2024. <br/>
  </p>
</footer>
  </body>
</html>