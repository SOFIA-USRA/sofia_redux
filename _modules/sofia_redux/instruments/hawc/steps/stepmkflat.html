<!DOCTYPE html>

<html lang="en" data-content_root="../../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sofia_redux.instruments.hawc.steps.stepmkflat &#8212; sofia_redux v1.3.4.dev38+g92ea2f4</title>
    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/bootstrap-sofia.css?v=3fe2c07e" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/plot_directive.css" />
    
    <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
    <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script src="../../../../../_static/documentation_options.js?v=6aa39468"></script>
    <script src="../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script type="text/javascript" src="../../../../../_static/sidebar.js"></script>
    <script type="text/javascript" src="../../../../../_static/copybutton.js"></script>
    <link rel="icon" href="../../../../../_static/redux.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,600' rel='stylesheet' type='text/css'/>

  </head><body>
<div class="topbar">
  <a class="brand" title="Documentation Home" href="../../../../../index.html"><span id="logotext1">SOFIA</span><span id="logotext2">Redux</span><span id="logotext3">:docs</span></a>
  <ul>
    <li><a class="homelink" title="SOFIA Homepage" href="https://irsa.ipac.caltech.edu/Missions/sofia.html"></a></li>
    <li><a title="General Index" href="../../../../../genindex.html">Index</a></li>
    <li><a title="Module Index" href="../../../../../py-modindex.html">Modules</a></li>
    <li>
      
      
<form action="../../../../../search.html" method="get">
  <input type="text" name="q" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
      
    </li>
  </ul>
</div>

<div class="related">
    <h3>Navigation</h3>
    <ul>
      <li>
	<a href="../../../../../index.html">sofia_redux v1.3.4.dev38+g92ea2f4</a>
	 &#187;
      </li>
      <li><a href="../../../../index.html" accesskey="U">Module code</a> &#187;</li>
      
       
    </ul>
</div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for sofia_redux.instruments.hawc.steps.stepmkflat</h1><div class="highlight"><pre>
<span></span># Licensed under a 3-clause BSD style license - see LICENSE.rst
&quot;&quot;&quot;Flat creation pipeline step.&quot;&quot;&quot;

import os
import re

from astropy import log
import numpy as np

from sofia_redux.instruments.hawc.stepmoparent import StepMOParent
from sofia_redux.instruments.hawc.steploadaux import StepLoadAux
from sofia_redux.instruments.hawc.datafits import DataFits

__all__ = [&#39;StepMkflat&#39;]


<div class="viewcode-block" id="StepMkflat">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepmkflat.StepMkflat.html#sofia_redux.instruments.hawc.steps.stepmkflat.StepMkflat">[docs]</a>
class StepMkflat(StepMOParent, StepLoadAux):
    &quot;&quot;&quot;
    Create a flat file from internal calibrator data.

    This step combines the internal calibrator observations
    (INTCALs) taken adjacent to a science observation, then multiplies
    by a master flat (skycal) to generate an observation flat
    (OFT) file for each input group of INTCALs.  The skycal
    file is generally a flat generated from a Chop-Scan observation
    of a bright source, divided by its own associated INTCAL.
    The INTCAL files are used to correct for temporal variations
    in detector responsivity, caused by changes in bias, background
    loading, and/or ADR control temperature; the skycal corrects for
    more static detector pixel response variations.

    Variances from the INTCAL files are also propagated into the
    output flat files.

    The input for this step is multiple demodulated files from
    internal calibrator observations (CALMODE = &#39;INT_CAL&#39; files),
    demodulated using the ‘mode_intcal’ pipeline mode.  Thus mode
    must set the following parameters for StepDemodulate:

    -  *l0method* = RE
    -  *chopavg* = True
    -  *phasefile* = 0.0

    The output for this step is a normalized INTCAL, one per input
    file, before combination and multiplication by the skycal.
    As a side effect, the observation flat files are also written
    to disk with product identifier &#39;OFT&#39;, in a folder designated
    by the &#39;flatoutfolder&#39; parameter. OFT files have R ARRAY GAIN,
    T ARRAY GAIN, R ARRAY GAIN VAR, T ARRAY GAIN VAR, R BAD PIXEL
    MASK, and T BAD PIXEL MASK image extensions.

    The mode_intcal pipeline ending in StepMkFlat should be run before
    the regular chop/nod pipeline to make the observation flats. Then, the
    regular pipeline can be run, making sure that StepFlat is configured
    to look in flatoutfolder for flatfiles, with the appropriate
    fitkeys settings to make sure the correct flat is paired with the
    observation.
    &quot;&quot;&quot;
<div class="viewcode-block" id="StepMkflat.setup">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepmkflat.StepMkflat.html#sofia_redux.instruments.hawc.steps.stepmkflat.StepMkflat.setup">[docs]</a>
    def setup(self):
        r&quot;&quot;&quot;
        Set parameters and metadata for the pipeline step.

        Output files have PRODTYPE = &#39;mkflat&#39;, and are named with
        the step abbreviation &#39;DCL&#39;.

        Parameters defined for this step are:

        flatoutfolder : str
            Path for the folder to write flat files to.  May be
            relative or absolute.  Set to an empty string to write to
            the same folder as the input file.
        groupkey : str
            Header keyword that must match across INTCAL files in
            order to group them together.  Typically set to FILEGPID.
        skip_start : int
            Chops to exclude from the beginning of the file.
        skip_end : int
            Chops to exclude from the end of the file.
        bad_dead : float
            Raw data threshold for dead pixels.
        bad_ramping : float
            Raw data threshold for ramping pixels.
        normstd : float
            Threshold to exclude pixels with high standard deviation.
        ynormlowlim : list of float
            Threshold to exclude pixels with low normalized signal, given
            for [R0, R1, T0].
        ynormhighlim : list of float
            Threshold to exclude pixels with high normalized signal, given
            for [R0, R1, T0].
        ttor : float
            Scale factor for the T array to the R array.
        scalfile : str
            Filename for auxiliary file(s). Can contain \* and ?
            wildcards to match multiple files to be selected using fitkeys.
        bkupscal : str
            Back up filename for auxiliary file(s). Can contain \*
            and ? wildcards to match multiple files to be selected using
            fitkeys.
        scalfitkeys : list of str
            List of header keys that need to match the scal
            data file.  These are only used if multiple files match
            the input INTCAL file.
        daterange : float
            If DATE-OBS is in scalfitkeys, files are matched within
            this many days.
        &quot;&quot;&quot;
        # Name of the pipeline reduction step
        self.name = &#39;mkflat&#39;
        self.description = &#39;Make INTCAL Flats&#39;

        # Shortcut for pipeline reduction step and identifier for
        # saved file names.
        self.procname = &#39;dcl&#39;

        # Clear Parameter list
        self.paramlist = []

        # Append parameters
        self.paramlist.append([&#39;flatoutfolder&#39;, &#39;&#39;,
                               &quot;Path for the folder to write flat &quot;
                               &quot;files to (&#39;&#39; means folder of &quot;
                               &quot;input file&quot;])
        self.paramlist.append([&#39;groupkey&#39;, &#39;FILEGPID&#39;,
                               &#39;Header keyword to match input &#39;
                               &#39;files to the same observation&#39;])
        self.paramlist.append([&#39;skip_start&#39;, 1,
                               &#39;Chops to exclude from the &#39;
                               &#39;beginning of the file&#39;])
        self.paramlist.append([&#39;skip_end&#39;, 1,
                               &#39;Chops to exclude from the &#39;
                               &#39;end of the file&#39;])
        self.paramlist.append([&#39;bad_dead&#39;, 10.,
                               &#39;Raw data threshold for dead pixels&#39;])
        self.paramlist.append([&#39;bad_ramping&#39;, 2e6,
                               &#39;Raw data threshold for ramping pixels&#39;])
        self.paramlist.append([&#39;normstd&#39;, 10.,
                               &#39;Threshold for HIGH STD of DMD &#39;
                               &#39;SIGNAL to exclude pixels&#39;])
        self.paramlist.append([&#39;ynormlowlim&#39;, [.5, .5, .5],
                               &#39;Threshold to eliminate pixels &#39;
                               &#39;with LOW SIGNAL&#39;])
        self.paramlist.append([&#39;ynormhighlim&#39;, [10., 10., 10.],
                               &#39;Threshold to eliminate pixels with &#39;
                               &#39;HIGH NORMALIZED SIGNAL&#39;])
        self.paramlist.append([&#39;TtoR&#39;, 2.0,
                               &#39;Scale factor for T/R flatfield&#39;])
        self.paramlist.append([&#39;dcl_only&#39;, False,
                               &#39;Make DCL output only (no OFT), and save &#39;
                               &#39;to flatoutfolder&#39;])

        # Get parameters for StepLoadAux, replace auxfile with scal
        self.loadauxsetup(auxpar=&#39;scal&#39;)</div>


<div class="viewcode-block" id="StepMkflat.run">
<a class="viewcode-back" href="../../../../../api/sofia_redux.instruments.hawc.steps.stepmkflat.StepMkflat.html#sofia_redux.instruments.hawc.steps.stepmkflat.StepMkflat.run">[docs]</a>
    def run(self):
        &quot;&quot;&quot;
        Run the data reduction algorithm.

        Because this step is multi-in, multi-out (MIMO),
        self.datain must be a list of DataFits objects.  The output
        is also a list of DataFits objects, stored in self.dataout.

        The process is:

        1. Read data from the INTCAL files.
        2. For each file, flag outliers and normalize by the median
           signal in the R and T arrays.
        3. Identify groups of data by matching across the &#39;groupkey&#39;
           parameter.
        4. Identify and read the data from an auxiliary skycal file.
        5. Mean-combine the normalized gain for each image in group.
        6. Multiply the mean INTCAL images by the skycal master
           flat.
        7. Save resulting OFT files to disk, in &#39;flatoutfolder&#39;.
        &quot;&quot;&quot;
        # Set Options and get parameters
        # Chops to exclude from the beginning of the file
        skip_start = self.getarg(&#39;skip_start&#39;)
        # Chops to exclude from the end of the file
        skip_end = self.getarg(&#39;skip_end&#39;)
        # Raw data threshold for dead pixles
        bad_dead = self.getarg(&#39;bad_dead&#39;)
        # Raw data threshold for ramping pixels. Set ramping as low as
        # possible, but if &lt; 5e5 you may start to lose some good pixels
        bad_ramping = self.getarg(&#39;bad_ramping&#39;)
        # threshold for HIGH STD of DMD SIGNAL to exclude pixels
        normstd = self.getarg(&#39;normstd&#39;)
        # threshold to elliminate pixels with LOW SIGNAL
        ynormlowlim = self.getarg(&#39;ynormlowlim&#39;)
        # threshold for pixels with HIGH NORMALIZED SIGNAL
        ynormhighlim = self.getarg(&#39;ynormhighlim&#39;)
        # Scale factor for T/R flatfield
        t_to_r = self.getarg(&#39;TtoR&#39;)
        # option to skip flat generation entirely and make the INTCAL
        # product only
        skip_flat = self.getarg(&#39;dcl_only&#39;)

        # Loop through input datasets
        self.dataout = []
        self.auxout = []
        for pd in self.datain:
            # Store R/T Real/Imag in indata
            end = len(pd.table) - skip_end
            signals = [&#39;R array&#39;, &#39;T array&#39;, &#39;R array Imag&#39;, &#39;T array Imag&#39;]
            indata = np.zeros((4, end - skip_start, 41, 64))
            invar = np.zeros((4, end - skip_start, 41, 64))
            for i in range(4):
                indata[i] = pd.table[signals[i]][skip_start:end, :, :]
                invar[i] = pd.table[signals[i] + &#39; VAR&#39;][skip_start:end, :, :]

            # Store R/T Raw averages in rawavg
            rawavg = np.zeros((2, end - skip_start, 41, 64))
            signals = [&#39;R array AVG&#39;, &#39;T array AVG&#39;]
            for i in [0, 1]:
                rawavg[i] = pd.table[signals[i]][skip_start:end, :, :]

            # Make raw stdevs
            # 2, 41, 64 array
            rawstd = np.nanstd(rawavg, axis=1)

            # Set Dead / Ramping pixels as NaN in indata
            # This next section finds and masks dead and
            # ramping pixels, so they won&#39;t bias the medians
            # calculated below. This may be redundant if they have
            # already been eliminated by the pipeline in StepNoah.
            # If so, it won&#39;t hurt to do it again, and it
            # allows tightening the criteria beyond those
            # set in StepNoah, if desired.
            nbad = np.ones((4, 41, 64))

            # Identify bad and ramping pixels by looking low and high rawstd
            # Dead pixels
            nbad[np.where(rawstd &lt; bad_dead)] = np.NaN
            # Ramping pixels
            nbad[np.where(rawstd &gt; bad_ramping)] = np.NaN

            # Mark all bad pixels with NaN in indata
            # This sets the T1 array signal values to nan.
            nbad[1, :, 32:64] = np.NaN
            nbad.shape = (4, 1, 41, 64)
            # multiply a (4, n, 41, 64) array by (4, 1, 41, 64) array
            indata *= nbad
            invar *= nbad

            # Calculate Modulus, phase and the medians for R and T array

            # initialize arrays
            modulus = np.zeros((2, indata.shape[1], 41, 64))
            phase = np.zeros((2, indata.shape[1], 41, 64))
            modvar = np.zeros((2, invar.shape[1], 41, 64))
            # phasevar = np.zeros((2, invar.shape[1], 41, 64))

            # Loop over R and T array
            for i in range(2):
                rld = indata[i]
                imd = indata[i + 2]
                rlv = invar[i]
                imv = invar[i + 2]

                # m = sqrt(r^2 + i^2)
                # Vm = (1/m^2)(r^2 Vr + i^2 Vi + 2 r i sqrt(Vr Vi))
                msq = (rld**2 + imd**2)
                modulus[i] = np.sqrt(msq)
                modvar[i] = (1 / msq) * (rld**2 * rlv
                                         + imd**2 * imv
                                         + 2 * rld * imd * np.sqrt(rlv * imv))

                # p = arctan(i / r)
                # Vp = (r^2 + i^2)^(-2) * (i^2 Vr + Vi - 2 sqrt(Vr Vi))
                # (phase variance not needed at this time)
                phase[i] = np.arctan(imd / rld)
                # phasevar[i] = (rld**2 + imd**2)^(-2) * \
                #               (imd**2 * rlv + imv - 2 * np.sqrt(rlv * imv))

            modmap = np.nanmedian(modulus, axis=1)
            phasemap = np.nanmedian(phase, axis=1)

            # variance of median is approximately pi/2 * variance of mean
            with np.errstate(invalid=&#39;ignore&#39;):
                modmapvar = (np.pi / 2.) \
                    * (np.nansum(modvar, axis=1)
                       / np.count_nonzero(~np.isnan(modulus), axis=1) ** 2)

            # Generate temporary subarrays to iterate over data
            subs = np.zeros((3, 41, 32))
            subs[0] = modmap[0, :, 0:32]
            subs[1] = modmap[0, :, 32:64]
            subs[2] = modmap[1, :, 0:32]

            # Set limits then iterate over them
            lowlim = (np.nanmedian(modmap[0, :, 0:32]) / 5,
                      np.nanmedian(modmap[1, :, 0:32]) / 5,
                      np.nanmedian(modmap[0, :, 32:64]) / 5)
            highlim = (200000., 200000., 200000.)
            outs = []
            for i in range(4):
                # Operates on the real parts of the signals
                outs = self._histogram3d(subs, lowlim, highlim)
                lowlim = outs[0] / 2
                highlim = 2 * outs[0]
            ymeds = outs[0]

            # Normalized modulus map and its stdev -&gt; ynorm, ynormstd

            # Make normalized signal maps
            # Alias modmap to ymean to make it easier to re-use
            # code for making flats
            ymean = modmap
            ynorm = np.zeros((2, 41, 64))
            ynorm[0, :, 0:32] = ymean[0, :, 0:32] / ymeds[0]
            ynorm[0, :, 32:64] = ymean[0, :, 32:64] / ymeds[1]
            ynorm[1, :, 0:32] = ymean[1, :, 0:32] / ymeds[2]
            ynorm[1, :, 32:64] = np.NaN

            # Make normalized std maps
            ystd = np.nanstd(modulus, axis=1)
            ynormstd = np.zeros((2, 41, 64))
            ynormstd[0, :, 0:32] = ystd[0, :, 0:32] / ymean[0, :, 0:32]
            ynormstd[0, :, 32:64] = ystd[0, :, 32:64] / ymean[0, :, 32:64]
            ynormstd[1, :, 0:32] = ystd[1, :, 0:32] / ymean[1, :, 0:32]

            # Create bad pixel map and flatfield
            # using various selection criteria

            # Badstack datacube holds information about
            # which bad-pixel criteria have been detected.
            badstack = np.zeros((6, 2, 41, 64))

            # Badstack will be flattened into badsum
            # below (by addition). In badsum, bits in
            # a binary number indicate which criteria
            # have been triggered.

            # Criteria (1/1) This criterion identifies DEAD
            # pixels by looking for those with essentially zero rawstd.
            ybad = np.zeros((2, 41, 64))
            # dead has already been set above in section creating nbad.
            with np.errstate(invalid=&#39;ignore&#39;):
                ybad[np.where(rawstd &lt; bad_dead)] = 1
            badstack[0] = ybad.copy()

            # Criteria (2/2) This criterion identifies RAMPING
            # pixels by looking for very high rawstd
            ybad = np.zeros((2, 41, 64))
            # ramping has already been set above in section creating nbad.
            ybad[np.where(rawstd &gt; bad_ramping)] = 2
            badstack[1] = ybad.copy()

            # Criteria (3/4) This criterion looks for very HIGH STD
            # of the demodulated signal.
            # May not be completely redundant with lower limit
            # on rawstd above. Need to check this with more data.
            ybad = np.zeros((2, 41, 64))
            with np.errstate(invalid=&#39;ignore&#39;):
                ybad[np.where(ynormstd &gt; normstd)] = 4
            badstack[2] = ybad.copy()

            # One could define a criterion that looks for low rawstd
            # not to identify dead pixels but to identify pixels
            # with very low signal
            # (e.g., those on the &quot;normal&quot; resistance part of the IV curve).
            # For example, Rbad[where(rstd &lt; 1e1)]=xx. However this
            # may be redundant with rnorm &lt; 0.x below. May want to take
            # a closer look at this later.

            # Criteria (4/8) This criterion attempts to identify
            # and eliminate pixels with very LOW SIGNAL
            # (e.g., those on the &quot;normal&quot; part of the IV curve).
            ybad = np.zeros((2, 41, 64))
            with np.errstate(invalid=&#39;ignore&#39;):
                ybad[0, :, 0:32][np.where(ynorm[0, :, 0:32]
                                          &lt; ynormlowlim[0])] = 8
                ybad[0, :, 32:64][np.where(ynorm[0, :, 32:64]
                                           &lt; ynormlowlim[1])] = 8
                ybad[1, :, 0:32][np.where(ynorm[1, :, 0:32]
                                          &lt; ynormlowlim[2])] = 8
            badstack[3] = ybad.copy()

            # Criteria (5/16) This criterion identifies pixels
            # with very HIGH NORMALIZED SIGNAL.
            # Need to check whether this might eliminate some &quot;good&quot; pixels.
            # Might not be needed. Might be redundant with
            # (and more prone to eliminate &quot;good&quot; pixels than)
            # the criterion on ynormstd above.
            ybad = np.zeros((2, 41, 64))
            with np.errstate(invalid=&#39;ignore&#39;):
                ybad[0, :, 0:32][np.where(ynorm[0, :, 0:32]
                                          &gt; ynormhighlim[0])] = 16
                ybad[0, :, 32:64][np.where(ynorm[0, :, 32:64]
                                           &gt; ynormhighlim[1])] = 16
                ybad[1, :, 0:32][np.where(ynorm[1, :, 0:32]
                                          &gt; ynormhighlim[2])] = 16
            badstack[4] = ybad.copy()

            # Criteria (6/32) This one ELIMINATES ROW 40
            # (the row with multiplexer but no active IR pixels).
            # This one may not be needed if the bad pixel mask ingested
            # at the beginning of this algorithm has already set
            # these rows to &quot;bad&quot;. On the other hand, this doesn&#39;t
            # take long to do here, so we could leave it
            # for now, just in case we change approaches later.
            ybad = np.zeros((2, 41, 64))
            ybad[:, 40, :] = 32
            badstack[5] = ybad.copy()

            # Create bad pixel map (value labels how the pixel is bad)
            # This step combines all the bad pixel identifications
            # into a single map with binary values for which the bits
            # each indicate that a particular selection criterion
            # has been triggered.
            badsum = np.sum(badstack, axis=0)

            # Create bad pixel mask (with nans) to apply
            # to flatfields. Add this to the target map
            # to mask the bad pixels.
            bad = np.zeros((2, 41, 64))
            with np.errstate(invalid=&#39;ignore&#39;):
                bad[np.where(badsum != 0)] = np.NaN

            # Make flatfields (Rcal, Tcal) and medians array

            # Make DCAL type flat fields, based on time-averaged
            # modulus (bad pixels set to NaN)
            rcal = 10000. / ymean[0] + bad[0]
            tcal = 10000. * t_to_r / ymean[1] + bad[1]

            # Propagate variance
            rvar = (10000. / (ymean[0]**2))**2 * modmapvar[0] + bad[0]
            tvar = (10000. * t_to_r / (ymean[1]**2))**2 * modmapvar[1] + bad[1]

            # Make an array to hold information on the
            # median intensities and masked map medians of
            # normalized std&#39;s, rawavg, and rawavg std&#39;s.
            medians = np.zeros((4, 3))
            medians[0] = ymeds

            ynormstdm = ynormstd + bad
            ynormstdmmed = np.zeros(3)
            ynormstdmmed[0] = np.nanmedian(ynormstdm[0, :, 0:32])
            ynormstdmmed[1] = np.nanmedian(ynormstdm[0, :, 32:64])
            ynormstdmmed[2] = np.nanmedian(ynormstdm[1, :, 0:32])
            medians[1] = ynormstdmmed

            rawmedianm = np.nanmedian(rawavg, axis=1) + bad
            rawmedianmmed = np.zeros(3)
            rawmedianmmed[0] = np.nanmedian(rawmedianm[0, :, 0:32])
            rawmedianmmed[1] = np.nanmedian(rawmedianm[0, :, 32:64])
            rawmedianmmed[2] = np.nanmedian(rawmedianm[1, :, 0:32])
            medians[2] = rawmedianmmed

            rawstdm = rawstd + bad
            rawstdmmed = np.zeros(3)
            rawstdmmed[0] = np.nanmedian(rawstdm[0, :, 0:32])
            rawstdmmed[1] = np.nanmedian(rawstdm[0, :, 32:64])
            rawstdmmed[2] = np.nanmedian(rawstdm[1, :, 0:32])
            medians[3] = rawstdmmed

            # Save to output object.

            # This will save a DCAL named after input file
            # with &#39;DCAL&#39; in file-type identifier field.
            # Rcal and Tcal correct detector gains to the
            # values that would give a signal of 10000 ADU
            # when exposed to the IR-50 internal calibrator source.
            # The factor TorR is a first-order correction for the
            # difference in the IR-50 illumination of the R and T
            # detectors. Higher-order corrections for the
            # detector to detector differences when exposed to
            # radiation from the sky must be derived from direct
            # measurements of astronomical sources.
            # t_to_r = 2.0 is a first-order guess based on
            # limited observations of astronomical source
            # signals. It could be refined by additional
            # observations, or the remaining differences could
            # be subsumed in the intcal-to-source-flat
            # transformation coefficients. The number 10000
            # was chosen to make the median R0 subarray signal
            # approximately equal to what it would be if no
            # corrections were applied (based on observations
            # with &quot;typical&quot; biases used in commissioning flights).

            # Get Configuration image from DMD file
            imgconfig = pd.imageget(&#39;CONFIGURATION&#39;)

            # Convert phases to time delay
            chopfreq = pd.getheadval(&#39;CHPFREQ&#39;)
            rphase = phasemap[0] * (-1 / (2 * np.pi * chopfreq)) + bad[0]
            tphase = phasemap[1] * (-1 / (2 * np.pi * chopfreq)) + bad[1]

            # Shift the phases to center around zero
            rphase = rphase - (-0.055)
            tphase = tphase - (-0.055)

            # Set NaN pixels to median
            # (not used at this time)

            # R0medianm = np.nanmedian(rphase[:, 0:32])
            # R1medianm = np.nanmedian(rphase[:, 32:64])
            # T0medianm = np.nanmedian(tphase[:, 0:32])
            # nanR0 = np.where(np.isnan(rphase[:, 0:32]))
            # nanT = np.where(np.isnan(tphase))
            # rphase[nanR0]=R0medianm
            # tphase[nanT]=T0medianm
            # nanR1 = np.where(np.isnan(rphase))
            # rphase[nanR1] = R1medianm

            # Make and fill output data
            outd = DataFits(config=pd.config)
            outd.filename = pd.filename
            outd.imageset(rcal, &#39;R ARRAY GAIN&#39;)
            outd.imageset(tcal, &#39;T ARRAY GAIN&#39;)
            outd.imageset(rvar, &#39;R ARRAY GAIN VAR&#39;)
            outd.imageset(tvar, &#39;T ARRAY GAIN VAR&#39;)
            outd.imageset(badsum[0], &#39;R BAD PIXEL MASK&#39;)
            outd.imageset(badsum[1], &#39;T BAD PIXEL MASK&#39;)
            outd.imageset(medians, &#39;SUBARRAY MEDIANS&#39;)
            outd.imageset(imgconfig, &#39;CONFIGURATION&#39;)
            outd.imageset(rphase, &#39;RPHASE&#39;)
            outd.imageset(tphase, &#39;TPHASE&#39;)
            outd.imageset(rawmedianm[0], &#39;R RAWAVG&#39;)
            outd.imageset(rawmedianm[1], &#39;T RAWAVG&#39;)
            outd.header = pd.header

            # Write the configuration header from pd into
            # the corresponding HDU in outd.
            outd.imgheads[5] = pd.imgheads[1]

            # Append to dataout
            self.dataout.append(outd)

        # END of loop over input datasets

        # Make groups of output files -&gt; datagroups
        groupkey = self.getarg(&#39;groupkey&#39;)
        datagroups = []

        # Make output folder if it doesn&#39;t exist
        outfolder = self.getarg(&#39;flatoutfolder&#39;)
        if len(outfolder) &gt; 0:
            if not os.path.isabs(outfolder):
                # if outfolder is not an absolute path, make it
                # relative to the output data location
                outfolder = os.path.join(
                    os.path.dirname(self.dataout[0].filename),
                    outfolder)
            try:
                os.makedirs(outfolder)
            except OSError:
                if not os.path.isdir(outfolder):
                    log.error(&#39;Run: Failed to make flatoutfolder = %s&#39; %
                              outfolder)
                    raise

        # if desired, just save DCL files to flats folder and quit
        if skip_flat:
            for data in self.dataout:
                dclname = data.filenamebegin + &#39;DCL&#39; + data.filenameend
                if len(outfolder) &gt; 0:
                    dclname = os.path.join(outfolder,
                                           os.path.basename(dclname))
                data.save(dclname)
                self.auxout.append(dclname)
                log.info(&#39;Saved result %s&#39; % dclname)
            self.dataout = []
            return

        # Loop over datasets in datain
        for data in self.dataout:
            groupind = 0
            # Loop over groups until group match found or end reached
            while groupind &lt; len(datagroups):
                # Check if data fits group: Get first group element
                gdata = datagroups[groupind][0]

                # Get key from group and new data - format if needed
                dkey = data.getheadval(groupkey)
                gkey = gdata.getheadval(groupkey)

                # Match -&gt; add to group
                if dkey == gkey:
                    datagroups[groupind].append(data)
                    break

                # Not found -&gt; increase group index
                groupind += 1
            # If not in any group -&gt; make new group
            if groupind == len(datagroups):
                datagroups.append([data, ])

        # info messages
        log.debug(&quot; Found %d data groups&quot; % len(datagroups))
        for groupind in range(len(datagroups)):
            group = datagroups[groupind]
            msg = &quot;  Group %d len=%d&quot; % (groupind, len(group))
            msg += &quot; %s = %s&quot; % (groupkey, group[0].getheadval(groupkey))
            log.debug(msg)

        # Make and save groupflats

        # loop over datagroups
        for groupind in range(len(datagroups)):
            data0 = datagroups[groupind][0]

            # Load masterflat for this datagroup -&gt; mflat
            mflat = self.loadauxfile(data=data0)

            # Average intcal R/T Array Gains
            filenum = []
            rlist = []
            tlist = []
            rvlist = []
            tvlist = []
            for data in datagroups[groupind]:
                rg = data.imageget(&#39;R ARRAY GAIN&#39;)
                if np.any(~np.isnan(rg)):
                    rlist.append(rg)
                    rvlist.append(data.imageget(&#39;R ARRAY GAIN VAR&#39;))
                    found_r = True
                else:
                    found_r = False

                tg = data.imageget(&#39;T ARRAY GAIN&#39;)
                if np.any(~np.isnan(tg)):
                    tlist.append(tg)
                    tvlist.append(data.imageget(&#39;T ARRAY GAIN VAR&#39;))
                    found_t = True
                else:
                    found_t = False

                if found_r or found_t:
                    if data.filenum is not None:
                        filenum.extend(data.filenum.split(&#39;-&#39;))
                else:
                    log.warning(&#39;Excluding bad file %s&#39; % data.filename)

            if not rlist or not tlist:
                msg = &#39;No good flat files found.&#39;
                log.error(msg)
                raise ValueError(msg)

            rgainavg = np.nanmean(np.array(rlist), axis=0)
            tgainavg = np.nanmean(np.array(tlist), axis=0)
            rgainvar = np.nansum(np.array(rvlist), axis=0) / len(rvlist) ** 2
            tgainvar = np.nansum(np.array(tvlist), axis=0) / len(tvlist) ** 2

            # Multiply masterflat -&gt; groupflat
            gflatr = mflat.imageget(&#39;R ARRAY GAIN&#39;) * rgainavg
            gflatt = mflat.imageget(&#39;T ARRAY GAIN&#39;) * tgainavg
            gflatrvar = (mflat.imageget(&#39;R ARRAY GAIN&#39;))**2 * rgainvar
            gflattvar = (mflat.imageget(&#39;T ARRAY GAIN&#39;))**2 * tgainvar

            # Groupflat bad pixel mask: see where there are NaNs
            gbadr = np.zeros(gflatr.shape)
            gbadt = np.zeros(gflatt.shape)
            gbadr[np.where(np.isnan(gflatr))] = 1.0
            gbadt[np.where(np.isnan(gflatt))] = 2.0

            # Make groupflat filename: Filename of
            # first file with OBSFLAT (OFT)
            gflatname = data0.filenamebegin + &#39;OFT&#39; + data0.filenameend

            # Split file, make sure outfolder is valid
            if len(outfolder) &gt; 0:
                gflatname = os.path.split(gflatname)[1]
            else:
                outfolder, gflatname = os.path.split(gflatname)

            # Update file number to be a range
            if len(filenum) &gt; 1:
                fn = sorted(filenum)
                filenums = fn[0] + &#39;-&#39; + fn[-1]
                match = re.search(self.config[&#39;data&#39;][&#39;filenum&#39;], gflatname)
                if match is not None:
                    # regex may contain multiple possible matches --
                    # for middle or end of filename
                    for i, g in enumerate(match.groups()):
                        if g is not None:
                            gbegin = gflatname[:match.start(i + 1)]
                            gend = gflatname[match.end(i + 1):]
                            gflatname = gbegin + filenums + gend
                            break
            gflatname = os.path.join(outfolder, gflatname)

            # Make groupflat data
            gflat = DataFits(config=self.datain[0].config)
            gflat.header = data0.header.copy()

            # Make header
            for data in datagroups[groupind][1:]:
                gflat.mergehead(data)
            for data in datagroups[groupind]:
                gflat.setheadval(&#39;PRODTYPE&#39;, &#39;obsflat&#39;)
                gflat.setheadval(&#39;PROCSTAT&#39;, &#39;LEVEL_2&#39;)
                gflat.setheadval(&#39;HISTORY&#39;, &#39;MakeFlat Source: %s&#39; %
                                 (os.path.split(data.filename)[1], ))
            # Fill data
            gflat.imageset(gflatr, &#39;R ARRAY GAIN&#39;)
            gflat.imageset(gflatt, &#39;T ARRAY GAIN&#39;)
            gflat.imageset(gflatrvar, &#39;R ARRAY GAIN VAR&#39;)
            gflat.imageset(gflattvar, &#39;T ARRAY GAIN VAR&#39;)
            gflat.imageset(gbadr, &#39;R BAD PIXEL MASK&#39;)
            gflat.imageset(gbadt, &#39;T BAD PIXEL MASK&#39;)

            # Save groupflat data
            gflat.save(gflatname)
            self.auxout.append(gflatname)
            log.info(&#39;Saved result %s&#39; % gflatname)</div>


    # fn &#39;histogram3d&#39; operates on a three-dimensional
    # image with multiple planes
    # (e.g., one containing both R and T array data).
    # Inputs are the following:
    #   cube = a three dimensional image
    #   hmin and hmax = lower and upper limits for the histogram
    #                   (one-dimensional arrays with
    #                    number of elements = &quot;planes&quot;).
    def _histogram3d(self, cube, hmin, hmax):
        &quot;&quot;&quot;
        Compute median and standard deviation of image planes.

        Parameters
        ----------
        cube : array-like
            A 3-dimensional image array, with distinct image planes.
        hmin : array-like of float
            Lower limit of the histogram, one value per plane.
        hmax : array-like of float
            Upper limit of the histogram, one value per plane

        Returns
        -------
        list of array-like
            The first element is the median value for each plane;
            second element is the standard deviation for each plane.
        &quot;&quot;&quot;
        planes = len(cube)
        medians = np.zeros(planes)
        stdof = np.zeros(planes)
        for i in range(planes):
            # Make a copy because we want to change the image
            # into a list without changing the original
            subarr = cube[i, ].copy()
            # Change the shape of subarr
            subarr.shape = (subarr.shape[0] * subarr.shape[1], )
            medians[i] = np.nanmedian([f for f in subarr if
                                       hmin[i] &lt;= f &lt; hmax[i]])
            stdof[i] = np.nanstd([f for f in subarr if
                                  hmin[i] &lt;= f &lt; hmax[i]])
        outdata = [medians, stdof]
        return outdata</div>

</pre></div>

            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Page Contents</h3>


        </div>
      </div>
      <div class="clearer"></div>
    </div>
<footer class="footer">
  <p class="pull-right"> &nbsp;
    <a href="#">Back to Top</a></p>
  <p>
    &copy; Copyright 2024, SOFIA-USRA.<br/>
    Created using <a href="http://www.sphinx-doc.org/en/stable/">Sphinx</a> 7.2.6. &nbsp;
    Last built 05 Feb 2024. <br/>
  </p>
</footer>
  </body>
</html>